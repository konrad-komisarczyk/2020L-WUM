---
title: "Milestone 2"
author: "Paweł Morgen, Dawid Przybyliński"
output: 
  html_document:
    df_print: kable
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
source("load.R")
```

## Inżynieria cech

Jak wspomniano na poprzednim kamieniu milowym, w zbiorze znajdują się zmienne wysoko skorelowane, a więc redundantne (np. odchylenie standardowe, współczynnik IQR, średnie odchylenie bezwzględne, kurtoza...). Redukcję przeprowadzono przy użyciu pakietu `caret`. Ponadto, w celu dalszej redukcji, szukano kombinacji liniowych (bez skutku) oraz zastosowano algorytm PCA.

```{r reduction, results='hold', cache=TRUE}
library(caret)

# Usuwamy skorelowane zmienne
corred <- findCorrelation(cor(X), cutoff = 0.8, names = FALSE)
cat(paste0("Ilość skorelowanych zmiennych do usunięcia: ", length(corred), " z ", ncol(X), "\n"))
# Usuwamy 389 zmiennych
X_uncorred <- X[,-corred]

# Usuwamy kombinacje liniowe innych zmiennych
linearCombos <- findLinearCombos(data.matrix(X_uncorred))

cat(paste0("Kombinacje liniowe do usunięcia: ", linearCombos$remove))

# PCA
prepPCA <- preProcess(X_uncorred, method = "pca")
X_PCAd <- predict(prepPCA, X_uncorred)
```

## Wstępna klasteryzacja

Znajomość metadanych zbioru danych pozwala określić, że szukamy **6** klastrów.

Skorzystamy z kilku metod: `kmeans`, `genie`oraz klastrowanie hierarchiczne Warda.

```{r preds, cache=TRUE}
preds_kmeans <- kmeans(X_PCAd, 6, iter.max = 100)
d <- dist(X_PCAd)
preds_genie <- genie::hclust2(d)
preds_hclust <- hclust(d, method = "ward.D2")
preds <- list(kmeans = preds_kmeans$cluster, 
              genie = cutree(preds_genie, 6), 
              ward = cutree(preds_hclust, 6))
```

### Weryfikacja i porównanie z oczekiwanymi etykietami

Do weryfikacji użyto średniej oraz odchylenia standardowego odległości między punktami w klastrze.

Do porównania użyto indeksów Randa oraz Fowlkesa-Mallowsa.

```{r measures, cache=TRUE}
mean_dist <- function(X, y){
  mean(sapply(split(1:nrow(X), as.factor(y)), function(i){
    mean(dist(X[i,]))
  }))
}

mean_sd <- function(X, y){
  mean(sapply(split(1:nrow(X), as.factor(y)), function(i){
    sd(dist(X[i,]))
  }))
}

df_ver <- data.frame(Algoritm = names(preds),
           mean_dist = sapply(preds, function(p) mean_dist(X_PCAd, p)),
           mean_sd = sapply(preds, function(p) mean_sd(X_PCAd, p)),
           FM_index = sapply(preds, function(p) dendextend::FM_index(p, y)),
           Rand_index = sapply(preds, function(p) mclust::adjustedRandIndex(p, y)))

df_ver
```

Jak łatwo zauważyć, wyniki nie są zadowalające. W przypadku algorytmów konwencjonalnych winna może być natura zbioru danych - jest to szereg czasowy, w którym numer obserwacji ma znaczenie. Powyższe algorytmy traktują obserwacje jak punkty w $R^n$ i tracą wymiar czasowy. 

### Inna ilość klastrów

Zweryfikowano skuteczność konwencjonalnych algorytmów dla innych ilości klastrów.

```{r another_ks, cache=TRUE, results='hold'}
ks <- 2:20
lapply(ks, function(k){
  p1 <- kmeans(X_PCAd, k, iter.max = 100)$cluster
  p2 <- cutree(preds_genie, k)
  p3 <- cutree(preds_hclust, k)
  data.frame(kmeans_FM = dendextend::FM_index(p1, y),
             genie_FM = dendextend::FM_index(p2, y),
             hclust_FM = dendextend::FM_index(p3, y),
             kmeans_silh = mean(cluster::silhouette(p1, dist = d)[,3]),
             genie_silh = mean(cluster::silhouette(p2, dist = d)[,3]),
             hclust_silh = mean(cluster::silhouette(p3, dist = d)[,3]))
}) -> res

bind_rows(res) %>%
  mutate(k = ks) %>%
  select(c(1:3, 7)) %>%
  tidyr::pivot_longer(1:3) %>%
  ggplot(aes(x = k, y = value, col = name)) +
  geom_line() +
  labs(title = "Goodness of fit with original labels") +
  ylab("FM index")

bind_rows(res) %>%
  mutate(k = ks) %>%
  select(c(4:7)) %>%
  tidyr::pivot_longer(1:3) %>%
  ggplot(aes(x = k, y = value, col = name)) +
  geom_line() +
  labs(title = "Silhouette score")
```

Wg wartości Silhouette najlepszą (!) ilością klastrów jest **2**. Jest to problematyczny rezultat.

### Komentarz

Zbiór danych powstawał na podstawie wartości sygnału urządzeń. Te sygnały w czasie nie miały wartości stałej. 

```{r ver1, echo=FALSE}
df <- data.frame(y = X[1:150, 1],
                 col = y[1:150],
                 x = 1:150)
ggplot(df, aes(x = x, y = y, col = col)) +
  geom_line() +
  geom_hline(yintercept = mean(df[1:25, 'y']), linetype = 'dashed') +
  labs(title = colnames(X)[1],
       subtitle = "First 150 observations")
```

Gdyby klasteryzację przeprowadzić wyłącznie na podstawie tej jednej zmiennej, punkty blisko czarnej prostej trafiłyby do jednego klastra. Jest to wynik niepożądany, jeśli zależy nam na zbliżeniu się do oryginalnych etykiet.

Przyjrzyjmy się, jak poszczególne algorytmy klasyfikowały tę przykładową zmienną:

```{r ver2, echo=FALSE}
cbind(df, as.data.frame(sapply(preds, factor))[1:150,]) %>%
  mutate(col = factor(as.integer(col))) %>%
  rename(original = col) %>%
tidyr::pivot_longer(cols = c(2, 4:6)) %>%
  ggplot(aes(x = x, y = y, col = value)) +
  geom_point() +
  facet_wrap(~name, ncol = 2)
```

## Podsumowanie

Nie ulega wątpliwości, że jest sporo miejsca do dalszej pracy. Jest to spowodowane m.in. niecodzienną naturą danych oraz oczekiwanym rezultatem - oczekujemy, że zamiast przypisywać klasy poszczególnym obserwacjom (i przypisywać je do fikcyjnych klastrów) podzielimy ciąg obserwacji na kawałki takie, że w każdym kawałku *trend* zmiennych będzie *wyglądać* podobnie.

Jest to zagadnienie poruszane w świecie ML, znane pod nazwą *Subsequence clustering*. Co zaskakujące, autorzy [tego](http://www.cs.ucr.edu/~eamonn/meaningless.pdf) artykułu naukowego argumentują, że takie zadanie jest... pozbawione matematycznego sensu.

Możliwe, że do osiągnięcia zamiaru klasteryzacji, dającego podobny podział, co oryginalne zmienne, wymagane jest przeformułowanie zadania i zbiorów danych.
